{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTcaLZaZ-lfb"
      },
      "source": [
        "# Autoregressive models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfaXF1hp_gKl"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "For the first part of this lab session, we will use textual data, since this will allow us to have faster training times, more efficient models and more enjoyable results. On the one hand, training RNNs on images is generally resource-heavy because of their high dimensionality and complex structure. On the other hand, we cannot just use vanilla RNN architectures for images, since these will produce unsatisfying results &ndash; you should rather resort to architectures should as [PixelRNN](https://arxiv.org/abs/1601.06759) or [DRAW](https://arxiv.org/abs/1502.04623), but these are challenging to train and out of scope for this lab session. And as will become clear later on in this course, autoregressive models are generally not the preferred family of models to generate images.\n",
        "\n",
        "We will use the data from (almost) all of **Shakespeare's plays**. We have already assembled them together in a single txt file by crawling [Project Gutenberg](https://www.gutenberg.org/). The entire file is approximately 6.1 MB large, and can be downloaded into the `content` folder of this Colab notebook by executing the following script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rmnv4ihe-_l_",
        "outputId": "a446a0b0-af7d-4e78-e874-9aaaa75bfb31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-26 14:24:35--  https://raw.githubusercontent.com/cedricdeboom/character-level-rnn-datasets/master/datasets/shakespeare.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6347705 (6.1M) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "shakespeare.txt     100%[===================>]   6.05M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-05-26 14:24:35 (187 MB/s) - ‘shakespeare.txt’ saved [6347705/6347705]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/cedricdeboom/character-level-rnn-datasets/master/datasets/shakespeare.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nierLQnhEYr3"
      },
      "source": [
        "In the first part of this lab session, we will train a so-called **character-level RNN**. This means that we will operate on the character level of the text, and not on the word level, i.e. a single token is a separate character in the text and we will generate the text one character at a time. We do this again to optimize the computational footprint and efficiency of the model, since there are much less unique characters than unique words in a text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFrhqd3_FohB"
      },
      "source": [
        "### Assignment 1\n",
        "\n",
        "Since neural networks only operate on numerical data, we have to be able to convert our text into such numerical format.\n",
        "\n",
        "1. Calculate the length (in no. of characters) of the entire dataset.\n",
        "2. Create a collection of all the unique characters in the dataset and calculate its size.\n",
        "3. Inspect the collection of unique characters. Are there any strange or unwanted characters? Remove them from the collection.\n",
        "4. Create two data structures that can be used to map each unique character onto a unique integer index, and vice versa. These will be used to convert between a text and a sequence of numbers.\n",
        "5. Since the dataset is not that large, we will keep the entire dataset in memory for quick access. Store the data as a single numerical (NumPy) array, thereby making use of the char-to-index map you created before. Make sure the array stores integers and not floats.\n",
        "\n",
        "If you want to further increase the efficiency of the model (and reduce the data dimensionality), you can first convert the entire dataset to lowercase letters, but this is not obligatory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7atUsseCEEk4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "CUDA_LAUNCH_BLOCKING=1    #Cuda set to synchronous\n",
        "torch.manual_seed(7777)\n",
        "np.random.seed(7777)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR9nxaaiKGYO",
        "outputId": "51fec7eb-fd3d-4086-aca8-1667500d2783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of total characters: 6347703\n"
          ]
        }
      ],
      "source": [
        "url = \"shakespeare.txt\"\n",
        "\n",
        "with open(url, 'r') as f:\n",
        "    text = f.read()\n",
        "n_char = [len(line) for line in text] \n",
        "print(\"Number of total characters: {}\".format(sum(n_char)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30Ud0v_vN7tb",
        "outputId": "a42238a4-5368-4566-b257-6aece30710b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of collection: 83\n",
            "﻿: 1\n",
            "1: 133\n",
            "6: 31\n",
            "0: 17\n",
            "3: 73\n",
            "\n",
            ": 156771\n",
            "A: 47187\n",
            "L: 23215\n",
            "S: 32543\n",
            " : 1423312\n",
            "W: 18635\n",
            "E: 38081\n",
            "T: 40896\n",
            "H: 18780\n",
            "N: 27633\n",
            "D: 13714\n",
            "b: 56691\n",
            "y: 103636\n",
            "i: 239083\n",
            "l: 176223\n",
            "a: 295203\n",
            "m: 115670\n",
            "h: 264508\n",
            "k: 35720\n",
            "e: 489267\n",
            "s: 259529\n",
            "p: 56224\n",
            "r: 253212\n",
            "t: 349659\n",
            "P: 11075\n",
            "o: 341127\n",
            "n: 259646\n",
            "K: 7326\n",
            "I: 58687\n",
            "G: 12182\n",
            "O: 30522\n",
            "F: 12050\n",
            "R: 26234\n",
            "C: 20529\n",
            "U: 15398\n",
            "B: 15127\n",
            "M: 15933\n",
            ",: 100379\n",
            "u: 138922\n",
            "f: 82885\n",
            "d: 162191\n",
            "w: 88783\n",
            "v: 40811\n",
            "g: 68965\n",
            "V: 3700\n",
            "c: 80790\n",
            ".: 84543\n",
            ":: 11966\n",
            ";: 20791\n",
            "': 36868\n",
            "j: 3253\n",
            "Y: 8546\n",
            "?: 12845\n",
            "-: 9655\n",
            "!: 10976\n",
            "x: 5089\n",
            "q: 2958\n",
            "[: 2071\n",
            "]: 2063\n",
            "z: 1430\n",
            "2: 81\n",
            "\": 450\n",
            "Q: 1408\n",
            "J: 2162\n",
            "4: 33\n",
            "5: 43\n",
            "Z: 730\n",
            "7: 9\n",
            "X: 492\n",
            "&: 24\n",
            "9: 26\n",
            "<: 28\n",
            "8: 6\n",
            "(: 122\n",
            "): 121\n",
            "}: 2\n",
            "_: 2\n",
            "$: 1\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "cnt = Counter(text)\n",
        "print(\"Size of collection: {}\".format(len(cnt)))\n",
        "for char, count in cnt.items():\n",
        "    print(f'{char}: {count}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSepOus3QAaQ"
      },
      "source": [
        "From the list above, characters "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m5hT4qZxP-Y_"
      },
      "outputs": [],
      "source": [
        "# Remove unwanted char\n",
        "# Remove unwanted char\n",
        "text = text.replace('\\n', ' ')\n",
        "text = text.replace('\\r', ' ')\n",
        "text = text.replace('\\ufeff', ' ')\n",
        "text = text.replace('$', '')\n",
        "text = text.replace('_', '')\n",
        "text = text.replace('-', '')\n",
        "text = text.replace('<', '')\n",
        "text = text.replace('{', '')\n",
        "text = text.replace('}', '')\n",
        "text = text.replace(']', '')\n",
        "text = text.replace('[', '')\n",
        "text = text.replace('1', '')\n",
        "text = text.replace('6', '')\n",
        "text = text.replace('0', '')\n",
        "text = text.replace('3', '')\n",
        "#text = text.pop('', None)\n",
        "\n",
        "\n",
        "#####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qstew6DfQV89",
        "outputId": "d2a190d5-8249-4911-e541-c0b3a23edf6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chars mapped to id: {'l': 0, 'k': 1, 'Y': 2, 'C': 3, 'D': 4, 'n': 5, 'A': 6, 'T': 7, 'b': 8, ' ': 9, ';': 10, 'c': 11, 'M': 12, 'w': 13, 'N': 14, 'L': 15, 'x': 16, '8': 17, 'E': 18, 'm': 19, 'F': 20, ')': 21, 's': 22, '2': 23, 'B': 24, 'd': 25, 'Q': 26, 'O': 27, ':': 28, 'r': 29, 'J': 30, 'j': 31, 'o': 32, '4': 33, 'X': 34, 'R': 35, 'W': 36, 'U': 37, 'p': 38, 'I': 39, '.': 40, \"'\": 41, '\"': 42, 'v': 43, 'z': 44, '&': 45, '!': 46, 'Z': 47, '7': 48, 'P': 49, 'a': 50, 'K': 51, 'f': 52, 'V': 53, 'h': 54, ',': 55, 'i': 56, 'g': 57, '?': 58, 'y': 59, '9': 60, 'e': 61, 'H': 62, 'q': 63, 'S': 64, '(': 65, 'G': 66, 't': 67, '5': 68, 'u': 69}\n",
            "Id mapped to char: {0: 'l', 1: 'k', 2: 'Y', 3: 'C', 4: 'D', 5: 'n', 6: 'A', 7: 'T', 8: 'b', 9: ' ', 10: ';', 11: 'c', 12: 'M', 13: 'w', 14: 'N', 15: 'L', 16: 'x', 17: '8', 18: 'E', 19: 'm', 20: 'F', 21: ')', 22: 's', 23: '2', 24: 'B', 25: 'd', 26: 'Q', 27: 'O', 28: ':', 29: 'r', 30: 'J', 31: 'j', 32: 'o', 33: '4', 34: 'X', 35: 'R', 36: 'W', 37: 'U', 38: 'p', 39: 'I', 40: '.', 41: \"'\", 42: '\"', 43: 'v', 44: 'z', 45: '&', 46: '!', 47: 'Z', 48: '7', 49: 'P', 50: 'a', 51: 'K', 52: 'f', 53: 'V', 54: 'h', 55: ',', 56: 'i', 57: 'g', 58: '?', 59: 'y', 60: '9', 61: 'e', 62: 'H', 63: 'q', 64: 'S', 65: '(', 66: 'G', 67: 't', 68: '5', 69: 'u'}\n"
          ]
        }
      ],
      "source": [
        "chars = set(text)\n",
        "#print(len(chars))\n",
        "\n",
        "char_to_id = {char:i for i, char in enumerate(chars)}\n",
        "id_to_char = {i:char for i, char in enumerate(chars)}\n",
        "\n",
        "print(\"Chars mapped to id: {}\".format(char_to_id))\n",
        "print(\"Id mapped to char: {}\".format(id_to_char))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CREMHYu7RyI7",
        "outputId": "557b3268-5367-4ec2-d2b1-d776c8dddf88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6333627,)\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "text_ids = [char_to_id[char] for char in text]\n",
        "\n",
        "data = np.array(text_ids, dtype=np.int32)\n",
        "print(data.shape)\n",
        "print(type(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aBB1N0wUG9G"
      },
      "source": [
        "## Representing and batching the data\n",
        "\n",
        "When we represent characters as integers, we implicitly define an ordering between the characters: character 0 is close to character 1, but farther away from character 35. This is unwanted: the distance between each character in the feature space should ideally be the same. For this reason we will use a so-called **one-hot encoding** of each character. This is a vector of all zeros, except for a 1 at the index position of the considered character. For example: if we have 4 unique characters, character 0 is one-hot encoded as `[1,0,0,0]`, character 1 as `[0,1,0,0]`, character 2 as `[0,0,1,0]` and character 3 as `[0,0,0,1]`.\n",
        "\n",
        "As you know, neural networks are (traditionally) trained with stochastic gradient descent. This means that the data must be delivered in batches at the input of the network. For sequential data this means that the data becomes 3-dimensional: if $B$ is the batch size, $T$ is the sequence length and $D$ is the data dimensionality, then each batch has shape $(B, T, N)$. Note that this immediately implies that within a batch, all sequences must have the same length $T$. Does is done by either:\n",
        "\n",
        " * Only taking chunks of length $T$ from the dataset to fill up the batch, or\n",
        " * Also taking chunks of length $\\leq T$, and filling up the remainder of the sequences with invalid data (padding).\n",
        "\n",
        "Since our entire dataset is essentially one long sequence, we can always sample chunks of length $T$, so we will take the (simpler) first approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbH56mYO5cFN"
      },
      "source": [
        "### Assignment 2\n",
        "\n",
        "Create a custom Dataset class for the Shakespeare data (see lab 1 for more details). For this purpose, write appropriate `__init__`, `__len__` and `__getitem__` methods. Make sure that you can specify the desired sequence length $T$ and data dimensionality $D$. You can take different approaches:\n",
        "\n",
        " * Divide the dataset in chunks of equal length $T$ e.g. by following the truncated backpropagation through-time (TBPTT) parameterizations (see lecture). You can then calculate how many sequences your dataset counts.\n",
        " * Return a random chunk whenever `__getitem__` is called. This is more versatile than the approach above and leads to smoother loss minimization, but you lose the notion of an \"epoch\". In this case, you can return whatever (large) number you want in the `__len__` method (e.g. `sys.maxsize` returns a large integer number).\n",
        "\n",
        "Remember that you also have to return the training target labels for a sequence in the `__getitem__` method! Think again about what the task is (\"predict the next character in a sequence\") and then decide what the labels should be. Again you have different possibilities (see lecture: single-loss training vs. multi-loss training). Let's pick **multi-loss training** for now: in that case the target labels should also be a sequence.\n",
        "\n",
        "Make sure that the input data is one-hot encoded (this is not needed for the target labels) and that your data has dtype float32 (single precision; which is the PyTorch default for neural network parameters)!\n",
        "\n",
        "Initialize an instance of a DataLoader and test your Dataset class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QTBhiZhrTsDk"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sys\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, seq_length, dim):\n",
        "      self.data = data\n",
        "      self.seq_length = seq_length\n",
        "      self.dim = dim\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      start_index = torch.randint(0, len(self.data) - self.seq_length - 1, (1,)).item()\n",
        "      \n",
        "      input_seq = torch.LongTensor(self.data[start_index:start_index+self.seq_length])\n",
        "      #print(type(input_seq))\n",
        "      \n",
        "      #One-hot encoding, hope casting to float() is the way to go (used it in the labs of ML-NLP and worked fine)\n",
        "      input_seq = F.one_hot(input_seq, num_classes = self.dim).float()\n",
        "\n",
        "      #Multi-loss training predicts for each char the next char, so the target for char i is i+1\n",
        "      target_seq = self.data[start_index+1:start_index+self.seq_length+1]\n",
        "      target_seq = torch.LongTensor(target_seq)\n",
        "\n",
        "      return input_seq, target_seq\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zjagoZp7WTTS",
        "outputId": "59b0daa6-87fe-4a64-90cd-f9ebadcd80f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 100, 128])\n",
            "torch.Size([1, 100])\n",
            "tensor([[[1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         ...,\n",
            "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 0., 0.,  ..., 0., 0., 0.],\n",
            "         [1., 0., 0.,  ..., 0., 0., 0.]]])\n",
            "tensor([[61, 67, 50,  0,  1, 10,  9,  9,  9,  9,  9,  7, 54, 61,  5,  9, 54, 32,\n",
            "         13, 22, 32, 19, 61, 41, 61, 29,  9, 67, 54, 32, 69,  9, 22, 38, 61, 50,\n",
            "          1, 41, 22, 67, 55,  9, 41, 19, 32,  5, 57,  9, 32, 67, 54, 61, 29,  9,\n",
            "         67, 54, 56,  5, 57, 22,  9,  9,  9,  9,  9, 39,  9, 22, 54, 50,  0,  0,\n",
            "          9, 25, 56, 57, 61, 22, 67,  9, 56, 67, 40,  9,  9,  9, 30, 18, 64, 64,\n",
            "         39,  3,  6, 40,  9, 36, 61,  0,  0, 55]])\n"
          ]
        }
      ],
      "source": [
        "dataset = TextDataset(data, 100, 128)\n",
        "dataloader = DataLoader(dataset)\n",
        "for i, (input_seq, target_seq) in enumerate(dataloader):\n",
        "    print(input_seq.shape)\n",
        "    print(target_seq.shape)\n",
        "    print(input_seq)\n",
        "    print(target_seq)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Sydo2crMm4g"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "We will now build the recurrent neural network in PyTorch that will be trained to predict the next character in a given sequence of characters. We will leverage GRU layers as the main building blocks, but you can experiment with other layers as well. You can find more information on Pytorch's recurrent layers on [https://pytorch.org/docs/master/nn.html#recurrent-layers](https://pytorch.org/docs/master/nn.html#recurrent-layers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg-THtIoXBDM"
      },
      "source": [
        "### Assignment 3\n",
        "\n",
        "A diagram of the envisioned architecture is shown in the picture below. Implement this architecture by inheriting from `torch.nn.Module`, as was explained in Lab 1. Test your module by getting a batch of data, feed it trough the network, and check if a sensible output is produced (especially if you have implemented the softmax - see remark 2 below - check if your output is properly normalized).\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=19Q4T7vhZtbdwrf4cS0SwPi3DqSZKqVSX' width='500' />\n",
        "\n",
        "The one-hot encoded input sequence is fed through two GRU layers with 128 hidden states. The outputs of the last GRU layer are fed through two dense layers. The first dense layer has a (leaky) ReLU output activation (but you can experiment with other ones as well). The final dense layer has an output dimensionality of $D$ and calculates a softmax over all possible characters. The parallel arrows indicate that each layer calculates an output at every time step. Of course, the dense layers do not have a recurrent nature and do not process a sequence in its entirety: each input of the dense layer is processed separately of the others and leads to its own output. But it was cleaner to draw the diagram this way.\n",
        "\n",
        "**Important remark 1**: The Pytorch recurrent layers have a `batch_first` argument that you ideally set to `True`. That way, the first data dimension is considered the batch dimension.\n",
        "\n",
        "**Important remark 2**: For the softmax nonlinearity, you can either choose to leave it out of the model during training and offload its computation to the CrossEntropyLoss object later (as done in Lab 1). Or you can choose to attach the log-softmax nonlinearity and make it part of the model. This will have implications on the loss function that will be used during training (see later). It's up to you, but please make sure that you know what you are doing!\n",
        "\n",
        "**Important remark 3**: The initial hidden state $\\mathbf{h}_0$ of your GRU layers is initialized as a vector of zeroes, as a PyTorch default. It is, however, possible to parameterize the initial hidden states and train these parameters; if you are interested in this, you can search through the documentation or on the internet how you can achieve this (not obligatory)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RdxV7tXhNG2U"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class model(nn.Module):\n",
        "  def __init__(self, input_size, output_size):\n",
        "    torch.nn.Module.__init__(self)\n",
        "    self.gru = nn.GRU(input_size= input_size, hidden_size= 128, num_layers=2, batch_first=True)\n",
        "    self.linear1 = nn.Linear(128, 128)\n",
        "    self.linear2 = nn.Linear(128, output_size)\n",
        "  \n",
        "  def forward(self,x, device, h_0=None):\n",
        "    if h_0 is None:\n",
        "      h_0 = self.init_hidden(x.size(0)).to(device)\n",
        "    x,h = self.gru(x, h_0)\n",
        "    x = F.relu(self.linear1(x))\n",
        "    x = self.linear2(x)   #Optional to add softmax here or after, from little bit of research, if using CrossEntropyLoss, more stable if using after\n",
        "    return x,h\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(2, batch_size, 128)\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-HRn9RvrOMn"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Now that we have the dataset and the model ready, it is time to train our very first character-level RNN!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YbFy9Ex9gAX"
      },
      "source": [
        "### Assignment 4\n",
        "\n",
        "Write an optimization procedure to train the RNN from assignment 3 using the data and batching strategy from assignments 1 and 2. We advise you to use the **Adam** optimizer with learning rate 0.001, which has become one of the default optimizers in deep learning, especially if you don't want to spend much time figuring out an effective learning rate schedule for plain SGD (which generally can lead to better optimizations). Pick a large enough batch size, e.g. 128, and a sequence length $T$ of around 100. Around 50 epochs of 1000 batches should be enough for now to train this model until \"reasonable\" convergence. Make sure to put the model and each batch on the GPU.\n",
        "\n",
        "**Important remark**: those of you who left out the log-softmax from the model, will need to use a CrossEntropyLoss. If you did use a log-softmax, you need a NLLLoss. Please read the documentation carefully regarding the use of these loss functions. Since we use **multi-loss training** we have a classification target at each time step, i.e. $T$ different loss values for each entry in the batch. Find out how to *correctly(!)* combine these losses into a single number (they can be averaged or summed, but make sure this is done along the correct axis). Another option (instead of using the built-in loss functions) is to write the loss criterion yourself in a separate function; this can be a nice exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uP8Toh-8CmmM",
        "outputId": "a6b5a5bf-20f5-4405-c9fb-37b6129048ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "use_cuda = True if torch.cuda.is_available() else False\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4BaoKzJ6m66c"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "batch_sz = 128\n",
        "seq_length = 100\n",
        "max_batches = 1000\n",
        "\n",
        "dataset = TextDataset(data, seq_length, batch_sz)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_sz)\n",
        "\n",
        "net = model(128,100).to(device)\n",
        "\n",
        "optimizer = Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(reduction='mean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGKNsALFC5xl",
        "outputId": "a9460336-5ea5-46cb-9c1f-54137a516c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100: Loss = 4.342677483558655\n",
            "Epoch 1, Batch 200: Loss = 3.875976686477661\n",
            "Epoch 1, Batch 300: Loss = 3.7517683839797975\n",
            "Epoch 1, Batch 400: Loss = 3.7676450753211976\n",
            "Epoch 1, Batch 500: Loss = 3.7094193267822266\n",
            "Epoch 1, Batch 600: Loss = 3.707845675945282\n",
            "Epoch 1, Batch 700: Loss = 3.6624151253700257\n",
            "Epoch 1, Batch 800: Loss = 3.7563997316360473\n",
            "Epoch 1, Batch 900: Loss = 3.6826087999343873\n",
            "Epoch 1, Batch 1000: Loss = 3.664598846435547\n",
            "Epoch 2, Batch 100: Loss = 3.6350302481651307\n",
            "Epoch 2, Batch 200: Loss = 3.627823338508606\n",
            "Epoch 2, Batch 300: Loss = 3.6303581166267396\n",
            "Epoch 2, Batch 400: Loss = 3.6155634331703186\n",
            "Epoch 2, Batch 500: Loss = 3.612919237613678\n",
            "Epoch 2, Batch 600: Loss = 3.6029789090156554\n",
            "Epoch 2, Batch 700: Loss = 3.603588674068451\n",
            "Epoch 2, Batch 800: Loss = 3.708564145565033\n",
            "Epoch 2, Batch 900: Loss = 3.6139603781700136\n",
            "Epoch 2, Batch 1000: Loss = 3.6174384641647337\n",
            "Epoch 3, Batch 100: Loss = 3.598064112663269\n",
            "Epoch 3, Batch 200: Loss = 3.594115710258484\n",
            "Epoch 3, Batch 300: Loss = 3.58249302148819\n",
            "Epoch 3, Batch 400: Loss = 3.6295399355888365\n",
            "Epoch 3, Batch 500: Loss = 3.6091846871376037\n",
            "Epoch 3, Batch 600: Loss = 3.601037836074829\n",
            "Epoch 3, Batch 700: Loss = 3.580625789165497\n",
            "Epoch 3, Batch 800: Loss = 3.6009621453285217\n",
            "Epoch 3, Batch 900: Loss = 3.580356571674347\n",
            "Epoch 3, Batch 1000: Loss = 3.5648205280303955\n",
            "Epoch 4, Batch 100: Loss = 3.570552408695221\n",
            "Epoch 4, Batch 200: Loss = 3.566980504989624\n",
            "Epoch 4, Batch 300: Loss = 3.555649733543396\n",
            "Epoch 4, Batch 400: Loss = 3.552317543029785\n",
            "Epoch 4, Batch 500: Loss = 3.5461984944343565\n",
            "Epoch 4, Batch 600: Loss = 3.552870185375214\n",
            "Epoch 4, Batch 700: Loss = 3.534750566482544\n",
            "Epoch 4, Batch 800: Loss = 3.523568549156189\n",
            "Epoch 4, Batch 900: Loss = 3.5324805665016172\n",
            "Epoch 4, Batch 1000: Loss = 3.537576014995575\n",
            "Epoch 5, Batch 100: Loss = 3.5395837092399596\n",
            "Epoch 5, Batch 200: Loss = 3.5265600728988646\n",
            "Epoch 5, Batch 300: Loss = 3.5036515831947326\n",
            "Epoch 5, Batch 400: Loss = 3.526369767189026\n",
            "Epoch 5, Batch 500: Loss = 3.493846118450165\n",
            "Epoch 5, Batch 600: Loss = 3.5360328483581545\n",
            "Epoch 5, Batch 700: Loss = 3.6156626629829405\n",
            "Epoch 5, Batch 800: Loss = 3.496997787952423\n",
            "Epoch 5, Batch 900: Loss = 3.4970247673988344\n",
            "Epoch 5, Batch 1000: Loss = 3.4825939989089965\n",
            "Epoch 6, Batch 100: Loss = 3.4926257634162905\n",
            "Epoch 6, Batch 200: Loss = 3.4817357158660887\n",
            "Epoch 6, Batch 300: Loss = 3.493359854221344\n",
            "Epoch 6, Batch 400: Loss = 3.4742105865478514\n",
            "Epoch 6, Batch 500: Loss = 3.4626416850090025\n",
            "Epoch 6, Batch 600: Loss = 3.4393487048149107\n",
            "Epoch 6, Batch 700: Loss = 3.518743052482605\n",
            "Epoch 6, Batch 800: Loss = 3.4837524032592775\n",
            "Epoch 6, Batch 900: Loss = 3.467893588542938\n",
            "Epoch 6, Batch 1000: Loss = 3.4345267248153686\n",
            "Epoch 7, Batch 100: Loss = 3.4998155355453493\n",
            "Epoch 7, Batch 200: Loss = 3.462458655834198\n",
            "Epoch 7, Batch 300: Loss = 3.4304576587677\n",
            "Epoch 7, Batch 400: Loss = 3.441689677238464\n",
            "Epoch 7, Batch 500: Loss = 3.430203297138214\n",
            "Epoch 7, Batch 600: Loss = 3.4235785150527955\n",
            "Epoch 7, Batch 700: Loss = 3.6419171619415285\n",
            "Epoch 7, Batch 800: Loss = 3.4516314005851747\n",
            "Epoch 7, Batch 900: Loss = 3.4851605796813967\n",
            "Epoch 7, Batch 1000: Loss = 3.4744506025314332\n",
            "Epoch 8, Batch 100: Loss = 3.4664174556732177\n",
            "Epoch 8, Batch 200: Loss = 3.4655176830291747\n",
            "Epoch 8, Batch 300: Loss = 3.4753901171684265\n",
            "Epoch 8, Batch 400: Loss = 3.4675798916816714\n",
            "Epoch 8, Batch 500: Loss = 3.4422330141067503\n",
            "Epoch 8, Batch 600: Loss = 3.4918057775497435\n",
            "Epoch 8, Batch 700: Loss = 3.4752901554107667\n",
            "Epoch 8, Batch 800: Loss = 3.4839966320991516\n",
            "Epoch 8, Batch 900: Loss = 3.4592385292053223\n",
            "Epoch 8, Batch 1000: Loss = 3.4944489073753355\n",
            "Epoch 9, Batch 100: Loss = 3.436648805141449\n",
            "Epoch 9, Batch 200: Loss = 3.459642403125763\n",
            "Epoch 9, Batch 300: Loss = 3.59139146566391\n",
            "Epoch 9, Batch 400: Loss = 3.469544777870178\n",
            "Epoch 9, Batch 500: Loss = 3.445218975543976\n",
            "Epoch 9, Batch 600: Loss = 3.555450556278229\n",
            "Epoch 9, Batch 700: Loss = 3.480184438228607\n",
            "Epoch 9, Batch 800: Loss = 3.4745987367630007\n",
            "Epoch 9, Batch 900: Loss = 3.4541516757011412\n",
            "Epoch 9, Batch 1000: Loss = 3.4309555864334107\n",
            "Epoch 10, Batch 100: Loss = 3.3765156126022338\n",
            "Epoch 10, Batch 200: Loss = 3.3420737838745116\n",
            "Epoch 10, Batch 300: Loss = 3.370546028614044\n",
            "Epoch 10, Batch 400: Loss = 3.337344958782196\n",
            "Epoch 10, Batch 500: Loss = 3.3705562567710876\n",
            "Epoch 10, Batch 600: Loss = 3.340931453704834\n",
            "Epoch 10, Batch 700: Loss = 3.3336746430397035\n",
            "Epoch 10, Batch 800: Loss = 3.3550864553451536\n",
            "Epoch 10, Batch 900: Loss = 3.3294845175743104\n",
            "Epoch 10, Batch 1000: Loss = 3.3187711596488954\n",
            "Epoch 11, Batch 100: Loss = 3.3290428376197814\n",
            "Epoch 11, Batch 200: Loss = 3.3203971457481383\n",
            "Epoch 11, Batch 300: Loss = 3.3162483763694763\n",
            "Epoch 11, Batch 400: Loss = 3.3111497378349304\n",
            "Epoch 11, Batch 500: Loss = 3.2982302832603456\n",
            "Epoch 11, Batch 600: Loss = 3.2938794445991517\n",
            "Epoch 11, Batch 700: Loss = 3.282668297290802\n",
            "Epoch 11, Batch 800: Loss = 3.3125519943237305\n",
            "Epoch 11, Batch 900: Loss = 3.3013179159164427\n",
            "Epoch 11, Batch 1000: Loss = 3.3007057237625124\n",
            "Epoch 12, Batch 100: Loss = 3.2806941962242124\n",
            "Epoch 12, Batch 200: Loss = 3.4589836025238037\n",
            "Epoch 12, Batch 300: Loss = 3.451213972568512\n",
            "Epoch 12, Batch 400: Loss = 3.433709955215454\n",
            "Epoch 12, Batch 500: Loss = 3.415682373046875\n",
            "Epoch 12, Batch 600: Loss = 3.406869695186615\n",
            "Epoch 12, Batch 700: Loss = 3.3768061566352845\n",
            "Epoch 12, Batch 800: Loss = 3.437458026409149\n",
            "Epoch 12, Batch 900: Loss = 3.3842607355117797\n",
            "Epoch 12, Batch 1000: Loss = 3.410619492530823\n",
            "Epoch 13, Batch 100: Loss = 3.3563104844093323\n",
            "Epoch 13, Batch 200: Loss = 3.3712812638282776\n",
            "Epoch 13, Batch 300: Loss = 3.440110931396484\n",
            "Epoch 13, Batch 400: Loss = 3.4635196805000303\n",
            "Epoch 13, Batch 500: Loss = 3.4550204062461853\n",
            "Epoch 13, Batch 600: Loss = 3.480087776184082\n",
            "Epoch 13, Batch 700: Loss = 3.494799690246582\n",
            "Epoch 13, Batch 800: Loss = 3.4602945470809936\n",
            "Epoch 13, Batch 900: Loss = 3.449675407409668\n",
            "Epoch 13, Batch 1000: Loss = 3.2934117245674135\n",
            "Epoch 14, Batch 100: Loss = 3.265236701965332\n",
            "Epoch 14, Batch 200: Loss = 3.270310196876526\n",
            "Epoch 14, Batch 300: Loss = 3.2504902601242067\n",
            "Epoch 14, Batch 400: Loss = 3.2596032214164734\n",
            "Epoch 14, Batch 500: Loss = 3.2511772108078003\n",
            "Epoch 14, Batch 600: Loss = 3.2431118583679197\n",
            "Epoch 14, Batch 700: Loss = 3.2399939560890196\n",
            "Epoch 14, Batch 800: Loss = 3.219253726005554\n",
            "Epoch 14, Batch 900: Loss = 3.2249468064308164\n",
            "Epoch 14, Batch 1000: Loss = 3.2724169921875\n",
            "Epoch 15, Batch 100: Loss = 3.207644021511078\n",
            "Epoch 15, Batch 200: Loss = 3.2493065357208253\n",
            "Epoch 15, Batch 300: Loss = 3.2050889110565186\n",
            "Epoch 15, Batch 400: Loss = 3.5409984517097475\n",
            "Epoch 15, Batch 500: Loss = 3.554127435684204\n",
            "Epoch 15, Batch 600: Loss = 3.540677194595337\n",
            "Epoch 15, Batch 700: Loss = 3.503171052932739\n",
            "Epoch 15, Batch 800: Loss = 3.4985056853294374\n",
            "Epoch 15, Batch 900: Loss = 3.4886575174331664\n",
            "Epoch 15, Batch 1000: Loss = 3.4808828091621398\n",
            "Epoch 16, Batch 100: Loss = 3.463696780204773\n",
            "Epoch 16, Batch 200: Loss = 3.447240948677063\n",
            "Epoch 16, Batch 300: Loss = 3.4529018211364746\n",
            "Epoch 16, Batch 400: Loss = 3.4417558526992797\n",
            "Epoch 16, Batch 500: Loss = 3.4472271275520323\n",
            "Epoch 16, Batch 600: Loss = 3.4179702591896057\n",
            "Epoch 16, Batch 700: Loss = 3.4127368402481078\n",
            "Epoch 16, Batch 800: Loss = 3.4238368248939515\n",
            "Epoch 16, Batch 900: Loss = 3.4330442643165586\n",
            "Epoch 16, Batch 1000: Loss = 3.42250048160553\n",
            "Epoch 17, Batch 100: Loss = 3.4181725120544435\n",
            "Epoch 17, Batch 200: Loss = 3.404083971977234\n",
            "Epoch 17, Batch 300: Loss = 3.4197000455856323\n",
            "Epoch 17, Batch 400: Loss = 3.4376516699790955\n",
            "Epoch 17, Batch 500: Loss = 3.388230173587799\n",
            "Epoch 17, Batch 600: Loss = 3.3485240530967713\n",
            "Epoch 17, Batch 700: Loss = 3.4410879802703858\n",
            "Epoch 17, Batch 800: Loss = 3.4019120216369627\n",
            "Epoch 17, Batch 900: Loss = 3.3887400007247925\n",
            "Epoch 17, Batch 1000: Loss = 3.3720322966575624\n",
            "Epoch 18, Batch 100: Loss = 3.3477729845046995\n",
            "Epoch 18, Batch 200: Loss = 3.7400642442703247\n",
            "Epoch 18, Batch 300: Loss = 3.501730854511261\n",
            "Epoch 18, Batch 400: Loss = 3.4704890513420104\n",
            "Epoch 18, Batch 500: Loss = 3.443546633720398\n",
            "Epoch 18, Batch 600: Loss = 3.4230473470687866\n",
            "Epoch 18, Batch 700: Loss = 3.4164681577682496\n",
            "Epoch 18, Batch 800: Loss = 3.400004312992096\n",
            "Epoch 18, Batch 900: Loss = 3.4001940941810607\n",
            "Epoch 18, Batch 1000: Loss = 3.389557673931122\n",
            "Epoch 19, Batch 100: Loss = 3.3811162948608398\n",
            "Epoch 19, Batch 200: Loss = 3.340090732574463\n",
            "Epoch 19, Batch 300: Loss = 3.3621815156936647\n",
            "Epoch 19, Batch 400: Loss = 3.404130387306213\n",
            "Epoch 19, Batch 500: Loss = 3.38842719078064\n",
            "Epoch 19, Batch 600: Loss = 3.411691107749939\n",
            "Epoch 19, Batch 700: Loss = 3.3955958175659178\n",
            "Epoch 19, Batch 800: Loss = 3.34956702709198\n",
            "Epoch 19, Batch 900: Loss = 3.327066595554352\n",
            "Epoch 19, Batch 1000: Loss = 3.2900795984268187\n",
            "Epoch 20, Batch 100: Loss = 3.3922374200820924\n",
            "Epoch 20, Batch 200: Loss = 3.360471546649933\n",
            "Epoch 20, Batch 300: Loss = 3.2281442785263064\n",
            "Epoch 20, Batch 400: Loss = 3.3098073267936705\n",
            "Epoch 20, Batch 500: Loss = 3.2483531761169435\n",
            "Epoch 20, Batch 600: Loss = 3.273567216396332\n",
            "Epoch 20, Batch 700: Loss = 3.259163258075714\n",
            "Epoch 20, Batch 800: Loss = 3.254490706920624\n",
            "Epoch 20, Batch 900: Loss = 3.2541628313064574\n",
            "Epoch 20, Batch 1000: Loss = 3.2526144576072693\n",
            "Epoch 21, Batch 100: Loss = 3.2475055718421935\n",
            "Epoch 21, Batch 200: Loss = 3.2444268679618835\n",
            "Epoch 21, Batch 300: Loss = 3.240524046421051\n",
            "Epoch 21, Batch 400: Loss = 3.233075954914093\n",
            "Epoch 21, Batch 500: Loss = 3.2541723442077637\n",
            "Epoch 21, Batch 600: Loss = 3.2401907658576965\n",
            "Epoch 21, Batch 700: Loss = 3.231444890499115\n",
            "Epoch 21, Batch 800: Loss = 3.2381224608421326\n",
            "Epoch 21, Batch 900: Loss = 3.236325213909149\n",
            "Epoch 21, Batch 1000: Loss = 3.2306791806221007\n",
            "Epoch 22, Batch 100: Loss = 3.2423148107528688\n",
            "Epoch 22, Batch 200: Loss = 3.540303966999054\n",
            "Epoch 22, Batch 300: Loss = 3.3081833481788636\n",
            "Epoch 22, Batch 400: Loss = 3.2806518363952635\n",
            "Epoch 22, Batch 500: Loss = 3.2591665530204774\n",
            "Epoch 22, Batch 600: Loss = 3.2662716126441955\n",
            "Epoch 22, Batch 700: Loss = 3.2237707448005675\n",
            "Epoch 22, Batch 800: Loss = 3.22144535779953\n",
            "Epoch 22, Batch 900: Loss = 3.5925776052474974\n",
            "Epoch 22, Batch 1000: Loss = 3.425550093650818\n",
            "Epoch 23, Batch 100: Loss = 3.374957039356232\n",
            "Epoch 23, Batch 200: Loss = 3.339016954898834\n",
            "Epoch 23, Batch 300: Loss = 3.338380973339081\n",
            "Epoch 23, Batch 400: Loss = 3.3617227911949157\n",
            "Epoch 23, Batch 500: Loss = 3.3180153369903564\n",
            "Epoch 23, Batch 600: Loss = 3.300348815917969\n",
            "Epoch 23, Batch 700: Loss = 3.3163080549240114\n",
            "Epoch 23, Batch 800: Loss = 3.3135505938529968\n",
            "Epoch 23, Batch 900: Loss = 3.245658311843872\n",
            "Epoch 23, Batch 1000: Loss = 3.2844040751457215\n",
            "Epoch 24, Batch 100: Loss = 3.3005863976478578\n",
            "Epoch 24, Batch 200: Loss = 3.2795031309127807\n",
            "Epoch 24, Batch 300: Loss = 3.348333435058594\n",
            "Epoch 24, Batch 400: Loss = 3.2763612723350524\n",
            "Epoch 24, Batch 500: Loss = 3.215613157749176\n",
            "Epoch 24, Batch 600: Loss = 3.3423563408851624\n",
            "Epoch 24, Batch 700: Loss = 3.3606138253211975\n",
            "Epoch 24, Batch 800: Loss = 3.282549834251404\n",
            "Epoch 24, Batch 900: Loss = 3.236534240245819\n",
            "Epoch 24, Batch 1000: Loss = 3.323818783760071\n",
            "Epoch 25, Batch 100: Loss = 3.3179502725601195\n",
            "Epoch 25, Batch 200: Loss = 3.362947952747345\n",
            "Epoch 25, Batch 300: Loss = 3.2296574306488037\n",
            "Epoch 25, Batch 400: Loss = 3.282968921661377\n",
            "Epoch 25, Batch 500: Loss = 3.2964090728759765\n",
            "Epoch 25, Batch 600: Loss = 3.3108197331428526\n",
            "Epoch 25, Batch 700: Loss = 3.4718347883224485\n",
            "Epoch 25, Batch 800: Loss = 3.36453572511673\n",
            "Epoch 25, Batch 900: Loss = 3.288468520641327\n",
            "Epoch 25, Batch 1000: Loss = 3.3487849044799805\n",
            "Epoch 26, Batch 100: Loss = 3.2475885128974915\n",
            "Epoch 26, Batch 200: Loss = 3.5641897201538084\n",
            "Epoch 26, Batch 300: Loss = 3.532436120510101\n",
            "Epoch 26, Batch 400: Loss = 3.517223858833313\n",
            "Epoch 26, Batch 500: Loss = 3.5214206910133363\n",
            "Epoch 26, Batch 600: Loss = 3.4986050367355346\n",
            "Epoch 26, Batch 700: Loss = 3.490759711265564\n",
            "Epoch 26, Batch 800: Loss = 3.4869938397407534\n",
            "Epoch 26, Batch 900: Loss = 3.476272282600403\n",
            "Epoch 26, Batch 1000: Loss = 3.4674037718772888\n",
            "Epoch 27, Batch 100: Loss = 3.494693839550018\n",
            "Epoch 27, Batch 200: Loss = 3.461045038700104\n",
            "Epoch 27, Batch 300: Loss = 3.4740625166893007\n",
            "Epoch 27, Batch 400: Loss = 3.4404457211494446\n",
            "Epoch 27, Batch 500: Loss = 3.4305966687202454\n",
            "Epoch 27, Batch 600: Loss = 3.4570661926269532\n",
            "Epoch 27, Batch 700: Loss = 3.440157163143158\n",
            "Epoch 27, Batch 800: Loss = 3.4137306213378906\n",
            "Epoch 27, Batch 900: Loss = 3.415243031978607\n",
            "Epoch 27, Batch 1000: Loss = 3.4022520756721497\n",
            "Epoch 28, Batch 100: Loss = 3.4402088809013365\n",
            "Epoch 28, Batch 200: Loss = 3.4413589882850646\n",
            "Epoch 28, Batch 300: Loss = 3.444862971305847\n",
            "Epoch 28, Batch 400: Loss = 3.4057974863052367\n",
            "Epoch 28, Batch 500: Loss = 3.3891904091835023\n",
            "Epoch 28, Batch 600: Loss = 3.358381223678589\n",
            "Epoch 28, Batch 700: Loss = 3.4087954688072206\n",
            "Epoch 28, Batch 800: Loss = 3.326756970882416\n",
            "Epoch 28, Batch 900: Loss = 3.4169447350502016\n",
            "Epoch 28, Batch 1000: Loss = 3.388887839317322\n",
            "Epoch 29, Batch 100: Loss = 3.3659722876548765\n",
            "Epoch 29, Batch 200: Loss = 3.4511629366874694\n",
            "Epoch 29, Batch 300: Loss = 3.379596972465515\n",
            "Epoch 29, Batch 400: Loss = 3.327335240840912\n",
            "Epoch 29, Batch 500: Loss = 3.3646873116493223\n",
            "Epoch 29, Batch 600: Loss = 3.340047924518585\n",
            "Epoch 29, Batch 700: Loss = 3.321354570388794\n",
            "Epoch 29, Batch 800: Loss = 3.304776225090027\n",
            "Epoch 29, Batch 900: Loss = 3.287379882335663\n",
            "Epoch 29, Batch 1000: Loss = 3.4764155173301696\n",
            "Epoch 30, Batch 100: Loss = 3.389542500972748\n",
            "Epoch 30, Batch 200: Loss = 3.3459673523902893\n",
            "Epoch 30, Batch 300: Loss = 3.385365533828735\n",
            "Epoch 30, Batch 400: Loss = 3.4299863290786745\n",
            "Epoch 30, Batch 500: Loss = 3.4139212012290954\n",
            "Epoch 30, Batch 600: Loss = 3.3668316769599915\n",
            "Epoch 30, Batch 700: Loss = 3.3473796319961546\n",
            "Epoch 30, Batch 800: Loss = 3.328859443664551\n",
            "Epoch 30, Batch 900: Loss = 3.315710244178772\n",
            "Epoch 30, Batch 1000: Loss = 3.3528268051147463\n",
            "Epoch 31, Batch 100: Loss = 3.3278820848464967\n",
            "Epoch 31, Batch 200: Loss = 3.3100970244407653\n",
            "Epoch 31, Batch 300: Loss = 3.2986472821235657\n",
            "Epoch 31, Batch 400: Loss = 3.2893294358253478\n",
            "Epoch 31, Batch 500: Loss = 3.332122519016266\n",
            "Epoch 31, Batch 600: Loss = 3.4620068144798277\n",
            "Epoch 31, Batch 700: Loss = 3.4456470584869385\n",
            "Epoch 31, Batch 800: Loss = 3.439287884235382\n",
            "Epoch 31, Batch 900: Loss = 3.4329019451141356\n",
            "Epoch 31, Batch 1000: Loss = 3.4243176865577696\n",
            "Epoch 32, Batch 100: Loss = 3.419257893562317\n",
            "Epoch 32, Batch 200: Loss = 3.417900626659393\n",
            "Epoch 32, Batch 300: Loss = 3.397803611755371\n",
            "Epoch 32, Batch 400: Loss = 3.3831108665466307\n",
            "Epoch 32, Batch 500: Loss = 3.3412861108779905\n",
            "Epoch 32, Batch 600: Loss = 3.355946378707886\n",
            "Epoch 32, Batch 700: Loss = 3.368655605316162\n",
            "Epoch 32, Batch 800: Loss = 3.3410651969909666\n",
            "Epoch 32, Batch 900: Loss = 3.3501723551750184\n",
            "Epoch 32, Batch 1000: Loss = 3.3433147621154786\n",
            "Epoch 33, Batch 100: Loss = 3.3137281465530397\n",
            "Epoch 33, Batch 200: Loss = 3.324601547718048\n",
            "Epoch 33, Batch 300: Loss = 3.321241614818573\n",
            "Epoch 33, Batch 400: Loss = 3.3593929195404053\n",
            "Epoch 33, Batch 500: Loss = 3.3514929366111756\n",
            "Epoch 33, Batch 600: Loss = 3.3467763805389406\n",
            "Epoch 33, Batch 700: Loss = 3.339211678504944\n",
            "Epoch 33, Batch 800: Loss = 3.3072740054130554\n",
            "Epoch 33, Batch 900: Loss = 3.3669297814369203\n",
            "Epoch 33, Batch 1000: Loss = 3.3389697885513305\n",
            "Epoch 34, Batch 100: Loss = 3.3362824869155885\n",
            "Epoch 34, Batch 200: Loss = 3.330424108505249\n",
            "Epoch 34, Batch 300: Loss = 3.3109732246398926\n",
            "Epoch 34, Batch 400: Loss = 3.294436421394348\n",
            "Epoch 34, Batch 500: Loss = 3.31519672870636\n",
            "Epoch 34, Batch 600: Loss = 3.33388206243515\n",
            "Epoch 34, Batch 700: Loss = 3.2925168871879578\n",
            "Epoch 34, Batch 800: Loss = 3.2752569007873533\n",
            "Epoch 34, Batch 900: Loss = 3.266876678466797\n",
            "Epoch 34, Batch 1000: Loss = 3.281231529712677\n",
            "Epoch 35, Batch 100: Loss = 3.2864154386520386\n",
            "Epoch 35, Batch 200: Loss = 3.276362507343292\n",
            "Epoch 35, Batch 300: Loss = 3.2637662196159365\n",
            "Epoch 35, Batch 400: Loss = 3.2501043558120726\n",
            "Epoch 35, Batch 500: Loss = 3.4712431740760805\n",
            "Epoch 35, Batch 600: Loss = 3.4425614976882937\n",
            "Epoch 35, Batch 700: Loss = 3.3925639605522155\n",
            "Epoch 35, Batch 800: Loss = 3.3884165596961977\n",
            "Epoch 35, Batch 900: Loss = 3.385431571006775\n",
            "Epoch 35, Batch 1000: Loss = 3.3785113978385923\n",
            "Epoch 36, Batch 100: Loss = 3.37527765750885\n",
            "Epoch 36, Batch 200: Loss = 3.381047477722168\n",
            "Epoch 36, Batch 300: Loss = 3.383471841812134\n",
            "Epoch 36, Batch 400: Loss = 3.3663354682922364\n",
            "Epoch 36, Batch 500: Loss = 3.371848158836365\n",
            "Epoch 36, Batch 600: Loss = 3.363596842288971\n",
            "Epoch 36, Batch 700: Loss = 3.3638257455825804\n",
            "Epoch 36, Batch 800: Loss = 3.3496554613113405\n",
            "Epoch 36, Batch 900: Loss = 3.375472249984741\n",
            "Epoch 36, Batch 1000: Loss = 3.344460361003876\n",
            "Epoch 37, Batch 100: Loss = 3.329525120258331\n",
            "Epoch 37, Batch 200: Loss = 3.3235849118232728\n",
            "Epoch 37, Batch 300: Loss = 3.3265587162971495\n",
            "Epoch 37, Batch 400: Loss = 3.4147143077850344\n",
            "Epoch 37, Batch 500: Loss = 3.321315770149231\n",
            "Epoch 37, Batch 600: Loss = 3.2419506883621216\n",
            "Epoch 37, Batch 700: Loss = 3.222442057132721\n",
            "Epoch 37, Batch 800: Loss = 3.381184980869293\n",
            "Epoch 37, Batch 900: Loss = 3.269444615840912\n",
            "Epoch 37, Batch 1000: Loss = 3.248135738372803\n",
            "Epoch 38, Batch 100: Loss = 3.2226877093315123\n",
            "Epoch 38, Batch 200: Loss = 3.20784503698349\n",
            "Epoch 38, Batch 300: Loss = 3.1899570631980896\n",
            "Epoch 38, Batch 400: Loss = 3.1910953521728516\n",
            "Epoch 38, Batch 500: Loss = 3.202494776248932\n",
            "Epoch 38, Batch 600: Loss = 3.179217870235443\n",
            "Epoch 38, Batch 700: Loss = 3.2087642669677736\n",
            "Epoch 38, Batch 800: Loss = 3.191497433185577\n",
            "Epoch 38, Batch 900: Loss = 3.179028823375702\n",
            "Epoch 38, Batch 1000: Loss = 3.201049556732178\n",
            "Epoch 39, Batch 100: Loss = 3.1896015501022337\n",
            "Epoch 39, Batch 200: Loss = 3.188920736312866\n",
            "Epoch 39, Batch 300: Loss = 3.171765916347504\n",
            "Epoch 39, Batch 400: Loss = 3.3691449666023257\n",
            "Epoch 39, Batch 500: Loss = 3.367373263835907\n",
            "Epoch 39, Batch 600: Loss = 3.3607563090324404\n",
            "Epoch 39, Batch 700: Loss = 3.3220307564735414\n",
            "Epoch 39, Batch 800: Loss = 3.335364964008331\n",
            "Epoch 39, Batch 900: Loss = 3.678631691932678\n",
            "Epoch 39, Batch 1000: Loss = 3.34939617395401\n",
            "Epoch 40, Batch 100: Loss = 3.2878893661499022\n",
            "Epoch 40, Batch 200: Loss = 3.3399491930007934\n",
            "Epoch 40, Batch 300: Loss = 3.2381663584709166\n",
            "Epoch 40, Batch 400: Loss = 3.1694742107391356\n",
            "Epoch 40, Batch 500: Loss = 3.1614023184776308\n",
            "Epoch 40, Batch 600: Loss = 3.143531358242035\n",
            "Epoch 40, Batch 700: Loss = 3.144150447845459\n",
            "Epoch 40, Batch 800: Loss = 3.129130549430847\n",
            "Epoch 40, Batch 900: Loss = 3.111298143863678\n",
            "Epoch 40, Batch 1000: Loss = 3.10677624464035\n",
            "Epoch 41, Batch 100: Loss = 3.2022120332717896\n",
            "Epoch 41, Batch 200: Loss = 3.1465668535232543\n",
            "Epoch 41, Batch 300: Loss = 3.13459153175354\n",
            "Epoch 41, Batch 400: Loss = 3.120337572097778\n",
            "Epoch 41, Batch 500: Loss = 3.1032258343696593\n",
            "Epoch 41, Batch 600: Loss = 3.1009396934509277\n",
            "Epoch 41, Batch 700: Loss = 3.095268313884735\n",
            "Epoch 41, Batch 800: Loss = 3.2461389470100404\n",
            "Epoch 41, Batch 900: Loss = 3.1149343061447143\n",
            "Epoch 41, Batch 1000: Loss = 3.076030249595642\n",
            "Epoch 42, Batch 100: Loss = 3.0823047947883606\n",
            "Epoch 42, Batch 200: Loss = 3.0874254441261293\n",
            "Epoch 42, Batch 300: Loss = 3.0762688755989074\n",
            "Epoch 42, Batch 400: Loss = 3.06874942779541\n",
            "Epoch 42, Batch 500: Loss = 3.403674478530884\n",
            "Epoch 42, Batch 600: Loss = 3.405635097026825\n",
            "Epoch 42, Batch 700: Loss = 3.372570767402649\n",
            "Epoch 42, Batch 800: Loss = 3.4436270904541018\n",
            "Epoch 42, Batch 900: Loss = 3.3740488266944886\n",
            "Epoch 42, Batch 1000: Loss = 3.311642897129059\n",
            "Epoch 43, Batch 100: Loss = 3.3048815369606017\n",
            "Epoch 43, Batch 200: Loss = 3.2978687119483947\n",
            "Epoch 43, Batch 300: Loss = 3.2223582339286803\n",
            "Epoch 43, Batch 400: Loss = 3.181638400554657\n",
            "Epoch 43, Batch 500: Loss = 3.1668577337265016\n",
            "Epoch 43, Batch 600: Loss = 3.1369015383720398\n",
            "Epoch 43, Batch 700: Loss = 3.145290973186493\n",
            "Epoch 43, Batch 800: Loss = 3.1141902661323546\n",
            "Epoch 43, Batch 900: Loss = 3.1255564069747925\n",
            "Epoch 43, Batch 1000: Loss = 3.0980918836593627\n",
            "Epoch 44, Batch 100: Loss = 3.0959273862838743\n",
            "Epoch 44, Batch 200: Loss = 3.331068034172058\n",
            "Epoch 44, Batch 300: Loss = 3.306739373207092\n",
            "Epoch 44, Batch 400: Loss = 3.296598792076111\n",
            "Epoch 44, Batch 500: Loss = 3.2745246744155883\n",
            "Epoch 44, Batch 600: Loss = 3.263528816699982\n",
            "Epoch 44, Batch 700: Loss = 3.3674965953826903\n",
            "Epoch 44, Batch 800: Loss = 3.479689829349518\n",
            "Epoch 44, Batch 900: Loss = 3.443561279773712\n",
            "Epoch 44, Batch 1000: Loss = 3.4292599606513976\n",
            "Epoch 45, Batch 100: Loss = 3.43207293510437\n",
            "Epoch 45, Batch 200: Loss = 3.405994424819946\n",
            "Epoch 45, Batch 300: Loss = 3.4138704323768616\n",
            "Epoch 45, Batch 400: Loss = 3.3832059502601624\n",
            "Epoch 45, Batch 500: Loss = 3.326312837600708\n",
            "Epoch 45, Batch 600: Loss = 3.323588387966156\n",
            "Epoch 45, Batch 700: Loss = 3.3152651238441466\n",
            "Epoch 45, Batch 800: Loss = 3.264448735713959\n",
            "Epoch 45, Batch 900: Loss = 3.2560938382148743\n",
            "Epoch 45, Batch 1000: Loss = 3.250933973789215\n",
            "Epoch 46, Batch 100: Loss = 3.3961202573776244\n",
            "Epoch 46, Batch 200: Loss = 3.3813174962997437\n",
            "Epoch 46, Batch 300: Loss = 3.4232420468330385\n",
            "Epoch 46, Batch 400: Loss = 3.4008421540260314\n",
            "Epoch 46, Batch 500: Loss = 3.3931502223014833\n",
            "Epoch 46, Batch 600: Loss = 3.3822967553138734\n",
            "Epoch 46, Batch 700: Loss = 3.375781202316284\n",
            "Epoch 46, Batch 800: Loss = 3.361779127120972\n",
            "Epoch 46, Batch 900: Loss = 3.3877254939079284\n",
            "Epoch 46, Batch 1000: Loss = 3.371010046005249\n",
            "Epoch 47, Batch 100: Loss = 3.3829854846000673\n",
            "Epoch 47, Batch 200: Loss = 3.3448320722579954\n",
            "Epoch 47, Batch 300: Loss = 3.297059540748596\n",
            "Epoch 47, Batch 400: Loss = 3.264289834499359\n",
            "Epoch 47, Batch 500: Loss = 3.3984564471244814\n",
            "Epoch 47, Batch 600: Loss = 3.351778464317322\n",
            "Epoch 47, Batch 700: Loss = 3.334792492389679\n",
            "Epoch 47, Batch 800: Loss = 3.3309091353416442\n",
            "Epoch 47, Batch 900: Loss = 3.290289254188538\n",
            "Epoch 47, Batch 1000: Loss = 3.317297739982605\n",
            "Epoch 48, Batch 100: Loss = 3.335137526988983\n",
            "Epoch 48, Batch 200: Loss = 3.267813055515289\n",
            "Epoch 48, Batch 300: Loss = 3.269696171283722\n",
            "Epoch 48, Batch 400: Loss = 3.2220287561416625\n",
            "Epoch 48, Batch 500: Loss = 3.234951024055481\n",
            "Epoch 48, Batch 600: Loss = 3.2190648174285887\n",
            "Epoch 48, Batch 700: Loss = 3.2961285877227784\n",
            "Epoch 48, Batch 800: Loss = 3.2929259943962097\n",
            "Epoch 48, Batch 900: Loss = 3.2730501961708067\n",
            "Epoch 48, Batch 1000: Loss = 3.257884268760681\n",
            "Epoch 49, Batch 100: Loss = 3.2447801208496094\n",
            "Epoch 49, Batch 200: Loss = 3.2231955623626707\n",
            "Epoch 49, Batch 300: Loss = 3.2041467308998106\n",
            "Epoch 49, Batch 400: Loss = 3.1760787391662597\n",
            "Epoch 49, Batch 500: Loss = 3.093030095100403\n",
            "Epoch 49, Batch 600: Loss = 3.0533380103111267\n",
            "Epoch 49, Batch 700: Loss = 3.0816339230537415\n",
            "Epoch 49, Batch 800: Loss = 3.0495539045333864\n",
            "Epoch 49, Batch 900: Loss = 3.0425719523429873\n",
            "Epoch 49, Batch 1000: Loss = 3.0518818283081055\n",
            "Epoch 50, Batch 100: Loss = 3.030911319255829\n",
            "Epoch 50, Batch 200: Loss = 3.186630883216858\n",
            "Epoch 50, Batch 300: Loss = 3.2186731123924255\n",
            "Epoch 50, Batch 400: Loss = 3.1753252029418944\n",
            "Epoch 50, Batch 500: Loss = 3.1666212463378907\n",
            "Epoch 50, Batch 600: Loss = 3.1510979890823365\n",
            "Epoch 50, Batch 700: Loss = 3.169063911437988\n",
            "Epoch 50, Batch 800: Loss = 3.130271701812744\n",
            "Epoch 50, Batch 900: Loss = 3.2942681169509886\n",
            "Epoch 50, Batch 1000: Loss = 3.3509798526763914\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 50\n",
        "running_loss = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (input_seq, target_seq) in enumerate(dataloader):\n",
        "    if i >= max_batches:\n",
        "      break\n",
        "    \n",
        "    input_seq = input_seq.to(device)\n",
        "    target_seq = target_seq.long().to(device)\n",
        "    #net.to(device)\n",
        "\n",
        "    net.train()      \n",
        "    optimizer.zero_grad()\n",
        "  \n",
        "    output_seq,_ = net(input_seq, device)\n",
        "    output_seq = output_seq.squeeze(2).to(device)\n",
        "    #print(output_seq.shape)\n",
        "    #print(target_seq.shape)\n",
        "\n",
        "    loss = criterion(output_seq, target_seq)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if i % 100 == 99:\n",
        "      print(f'Epoch {epoch+1}, Batch {i+1}: Loss = {running_loss/100}')\n",
        "      running_loss = 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J-OvdPFZpp5M"
      },
      "outputs": [],
      "source": [
        "torch.save(net.state_dict(), \"./model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXmOo8FCgfUn"
      },
      "source": [
        "## Generating text\n",
        "\n",
        "In the previous assignment, our RNN was trained to predict the next character in a given sequence. We will now use this trained RNN in generator mode to produce text on its own. Since we have used multi-loss training, we have multiple options for the sampling strategy: either progressive sampling or windowed sampling, which each have their own benefits and flaws (see lecture). Also, it often depends on the framework that you use (in our case PyTorch) which of the sampling strategies is more easy to implement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYs5DWpQijKt"
      },
      "source": [
        "### Assignment 5\n",
        "\n",
        "We will implement **progressive sampling**. We will start from a seed sequence of 100 characters after which we let the RNN generate the subsequent characters ad libitum (you can choose how many characters you want the RNN to generate). We also want to be able to tune the randomness in the model by means of a **softmax temperature** parameter. The plan is as follows:\n",
        "\n",
        " 1. Sample a random seed sequence of $T$ characters from the dataset and feed it through the RNN.\n",
        " 2. Consider the softmax output of the final time step, and use it to sample the next character in the sequence (`torch.multinomial` might come in handy).\n",
        " 3. Feed the recently sampled character through the RNN, but make sure that the RNN starts from the last hidden state of step 1! For this to work, you will need to change the model definition such that you can specify the initial hidden state, and such that the final hidden state can be captured when a sequence is fed through the RNN. Look in the documentation on how to achieve this.\n",
        " 4. Iterate from step 2 until enough characters are sampled.\n",
        "\n",
        "Test your sampler with different temperature values and different seed sequences. If all went right, observe that the model has learned to create words, separated by spaces, and that it has learned that there are character roles which are often written in uppercase letters. Also observe that the RNN is pretty good at generating language on a low level, but that it fails to produce coherent texts on a larger scale.\n",
        "\n",
        "**Important remark 1**: make sure that your input data still has a batch dimension. If this is not the case, take a look at the `unsqueeze` function to add extra dimensions to your data.\n",
        "\n",
        "**Important remark 2**: you will have to change the `forward`-method of the model class such that a softmax temperature can be specified as an extra argument.\n",
        "\n",
        "**Important remark 3**: since you will be changing the model class, it is advised to store the parameters of the already trained RNN. You can then load these parameters into your updated model object.\n",
        "\n",
        "**Important remark 4**: if you specify a random seed, your results will be reproducible, which might be handy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CHYp_J4ax3X"
      },
      "source": [
        "1. Get initial index as random number between 0 and length of data - T - 1\n",
        "2. From this first index you get the following consecutive T elements\n",
        "3. As the previous list will be a list of numbers, each corresponding to a char, you need to one-hot encode this list\n",
        "4. Then this sequence is feeded to the model (so it is predicting the next char from the previous list)\n",
        "5. The output sequence is divided by the temperature (so data is smoothed) and feeded to softmax \n",
        "6. Then can't I just get the max arg of the softmax, why exactly is torch.multinomial needed? sample from the distribution better results, but softmax should work\n",
        "7. Then the softmax will give us the position of the char in data \n",
        "8. From the position we can get its corresponding value, and so the corresponding char from its id\n",
        "9. And then the loop starts again, however only feeding the next generated char."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8X-JJ4WTqBjm",
        "outputId": "52d136ef-566b-412f-b9d0-2e38f73dd5c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "inference_model = model(128,100).to(device)\n",
        "inference_model.load_state_dict(torch.load(\"model.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dkNIczwjax3X"
      },
      "outputs": [],
      "source": [
        "#device = torch.device(\"cpu\")\n",
        "#print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8nqd1oiJDuly"
      },
      "outputs": [],
      "source": [
        "def progressive_sampling(rnn, dataset, T, num_samples, temperature, device):\n",
        "  rnn.eval()    \n",
        "  \n",
        "  start_index = torch.randint(0, len(dataset.data) - T - 1, (1,)).item()\n",
        "  seed_seq = dataset.data[start_index:start_index+T]\n",
        "  \n",
        "  seed_seq = torch.LongTensor(seed_seq).to(device)\n",
        "  \n",
        "  seed_seq = F.one_hot(seed_seq, dataset.dim).float()   #Model needs one-hot enc. of data\n",
        "  #print(\"Seed sequence shape: \" + str(seed_seq.shape))  # = (100,128)\n",
        "  \n",
        "  output_seq,hn = rnn(seed_seq.unsqueeze(0), device)    # seed_seq-> (1,100,128)\n",
        "  #print(\"Output sequence shape: \" + str(output_seq.shape))  # = (1,100,100)\n",
        "\n",
        "  softmax_output = F.softmax(output_seq/temperature, dim=2)  #Temperature is used for smoothing, so higher value more uniform dist.\n",
        "  #print('Softmax output shape: ' + str(softmax_output))     # = (1,100,100)\n",
        "\n",
        "  #next_char = torch.multinomial(softmax_output.squeeze(1),1)   #Multinomial sampling, so we get a random char from the distribution\n",
        "  next_char = softmax_output.argmax()\n",
        "  #print(\"Next char index: \" + str(next_char))\n",
        "\n",
        "  next_char_id = data[int(next_char)]\n",
        "  #print(\"Next char id: \" + str(next_char_id))\n",
        "  \n",
        "\n",
        "  #print(\"Character generated: \"+id_to_char[next_char_id])  \n",
        "  generated_seq = [id_to_char[next_char_id]]\n",
        "\n",
        "  for i in range(0,num_samples-1):\n",
        "    next_char_id = torch.LongTensor([next_char_id]).to(device)\n",
        "\n",
        "    one_hot_char = F.one_hot(next_char_id, dataset.dim).float()\n",
        "    #print(\"One hot shape: \" + str(one_hot_char.unsqueeze(0).shape)) # = (1,1,128)\n",
        "\n",
        "    output_seq,hn = rnn(one_hot_char.unsqueeze(0), device, hn)\n",
        "    #print(\"Output seq shape: \" + str(output_seq))\n",
        "    \n",
        "    softmax_output = F.softmax(output_seq/temperature, dim=2)\n",
        "    #print('Softmax output shape: ' + str(softmax_output.squeeze(1).squeeze(0).shape))   # = (1,1,100)\n",
        "    \n",
        "    next_char = torch.multinomial(softmax_output.squeeze(1).squeeze(0), 1)    #=(100)\n",
        "    #next_char = softmax_output.argmax()\n",
        "    #print(\"Next char index: \" + str(next_char))\n",
        "\n",
        "    next_char_id = data[int(next_char)]\n",
        "    #print(\"Character generated: \"+id_to_char[next_char_id])  \n",
        "\n",
        "    generated_seq.append(id_to_char[next_char_id])\n",
        "\n",
        "  return ''.join(generated_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MPOwpf08rQjO",
        "outputId": "f5d5c6b7-22dd-460e-94f0-56b403cbf111",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E                                                                                                   \n"
          ]
        }
      ],
      "source": [
        "print(progressive_sampling(inference_model, dataset, 100, 100, 1, device))\n",
        "#print(progressive_sampling(net, dataset, 100, 100, 1, device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRXkmnF6KLwp"
      },
      "source": [
        "## Evaluating generative language models\n",
        "\n",
        "Generative models can be evaluated by looking at either the likelihood of the predictions, or by looking at the sample quality. Both methods have their pros and cons &ndash; and as we will see in the lecture about generative adversarial networks, they are not necessarily correlated! &ndash; and usually a combination of the two is used in literature. \n",
        "\n",
        "In the previous assignment we have looked at (subjective) sample quality, in the following assignment we will look at a likelihood-based metric. For this metric, it is important that we calculate it on a separate test set. After all, at this point we don't know if the RNN has learned all training data by heart, or if it can truly generalise on unseen data from the true data distribution.\n",
        "\n",
        "A popular performance measure for language models is the **perplexity**:\n",
        "$$\\text{perplexity} = \\exp\\left(  \\frac{-\\sum_{t=1}^N \\log p(x_t \\vert x_{1:t-1})}{N} \\right)$$\n",
        "To calculate perplexity, we feed the entire test set ($N$ tokens in total) through the RNN and we calculate the log-likelihood of each ground-truth token. Lower perplexity means better model performance on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2R6iukKi58q"
      },
      "source": [
        "### Assignment 6\n",
        "\n",
        "1. Split the original dataset into a train and test set. Take around 10,000 characters for the test set (e.g. the last part of the dataset).\n",
        "\n",
        "2. Write a routine that calculates perplexity on this held-out test set. Start by feeding in the first character of the test set into the RNN, record the log-likelihood, and then iterate by going through the entire test sequence. Think about how you can make this routine as fast and efficient as possible (where do you put the data (cpu/gpu), when do you convert it to one-hot, etc.).\n",
        "\n",
        "3. Retrain the RNN and record perplexity on the test set after every epoch. Visualize the train and test metrics on a Tensorboard. Do you observe signs of overfitting? Or can we manage more than 50 training epochs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "oFB8v4R3kdXz"
      },
      "outputs": [],
      "source": [
        "#Split dataset into train and test\n",
        "test_data = data[-1000:]\n",
        "train_data = data[:-1000]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "tkyanyJRax3Z"
      },
      "outputs": [],
      "source": [
        "def perplexity(net, test_data, dim, device, k=0.2):\n",
        "    test_data = F.one_hot(torch.LongTensor(test_data), dim).float().to(device)\n",
        "    #print(test_data.shape)\n",
        "    net.train(False)\n",
        "    log_likelihood = 0.0\n",
        "    \n",
        "    #hn = net.init_hidden()\n",
        "    for i in range(0,len(test_data)-1):\n",
        "        #print(test_data[i].unsqueeze(0).unsqueeze(0).shape)\n",
        "        output_seq,_ = net(test_data[i].unsqueeze(0).unsqueeze(0), device)\n",
        "\n",
        "        #output_seq = (output_seq + k) / (output_seq.sum() + k * dim)\n",
        "        #                           output_seq(only vocab size))      indexes resulting tensor \n",
        "        log_likelihood += torch.log(torch.clamp(output_seq.squeeze(0).squeeze(0)[test_data[i+1].argmax()], min=1e-9)) \n",
        "        #print(log_likelihood)\n",
        "        #print(output_seq.squeeze(0).squeeze(0)[test_data[i+1].argmax()])\n",
        "    \n",
        "    net.train(True)\n",
        "    perp = torch.exp(-log_likelihood/len(test_data))\n",
        "  \n",
        "    return perp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Yg9hkEAgax3Z",
        "outputId": "fe50075c-9ca0-41e9-e42b-aac20b136007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 0.06222238391637802\n"
          ]
        }
      ],
      "source": [
        "perp = perplexity(inference_model, test_data, dataset.dim, device)\n",
        "print(\"Perplexity: \" + str(perp.item()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "U_gTRHCTax3a"
      },
      "outputs": [],
      "source": [
        "writer = SummaryWriter('runs/shakespeare_rnn_3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "1UbNyg60ax3a"
      },
      "outputs": [],
      "source": [
        "batch_sz = 128\n",
        "seq_length = 100\n",
        "max_batches = 1000\n",
        "\n",
        "train_dataset = TextDataset(train_data, seq_length, batch_sz)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_sz)\n",
        "\n",
        "model_q6 = model(128,100).to(device)\n",
        "\n",
        "optimizer = Adam(model_q6.parameters(), lr=0.001)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(reduction='mean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "QLgYUBOxax3g",
        "outputId": "0ca1517b-cb39-4c6a-f9b9-7d90c0420402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100: Loss = 3.536132731437683\n",
            "Epoch 1, Batch 200: Loss = 3.522833173274994\n",
            "Epoch 1, Batch 300: Loss = 3.5363314270973207\n",
            "Epoch 1, Batch 400: Loss = 3.5173425245285035\n",
            "Epoch 1, Batch 500: Loss = 3.5065287017822264\n",
            "Epoch 1, Batch 600: Loss = 3.514541211128235\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-439edd6d70e1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 50\n",
        "running_loss = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (input_seq, target_seq) in enumerate(train_dataloader):\n",
        "    if i >= max_batches:\n",
        "      break\n",
        "    \n",
        "    #print(input_seq)\n",
        "    #print(target_seq)\n",
        "    input_seq = input_seq.to(device)\n",
        "    target_seq = target_seq.long().to(device)\n",
        "    #net.to(device)\n",
        "\n",
        "    model_q6.train(True)      \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  \n",
        "    output_seq,_ = model_q6(input_seq, device)\n",
        "    output_seq = output_seq.squeeze(2).to(device)\n",
        "    #print(output_seq.shape)\n",
        "    #print(target_seq.shape)\n",
        "\n",
        "    loss = criterion(output_seq, target_seq)\n",
        "    \n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    if i % 100 == 99:\n",
        "      print(f'Epoch {epoch+1}, Batch {i+1}: Loss = {running_loss/100}')\n",
        "      writer.add_scalar('training loss', running_loss/100, epoch * len(train_dataloader) + i)\n",
        "      running_loss = 0.0\n",
        "\n",
        "  perp = perplexity(model_q6,test_data, dataset.dim, device)  \n",
        "  print('Perplexity: ' + str(perp.item()))\n",
        "  writer.add_scalar('perplexity', perp.item(), epoch * len(train_dataloader) + i)\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BDvY0-cax3h"
      },
      "source": [
        "Initial evaluation (50 epochs):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA0bXK7xax3h"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "![image-2.png](attachment:image-2.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-h0kAGDax3h"
      },
      "source": [
        "From the graphs we can conclude that the model is in fact learning as perplexity and loss decrases for each epoch. This is not as clear only analyzing the loss as the loss is not as smooth, but perplexity helps visualize this better. However I do think the model can be furthered trained as the loss and perplexity are still decreasing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d98cZw46Hlqj"
      },
      "source": [
        "## Extra ideas\n",
        "\n",
        "You are now finished with the lab session. If you want to do some more experiments, you could try:\n",
        "\n",
        " * Alter the model architecture: try LSTM instead of GRU, add or remove some of the recurrent layers, play with the dimensionality of the layers, etc.\n",
        " * Insert a so-called [embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding) after the input layer. This layer takes integers as input, so you will need to get rid of the one-hot representations. Alternatively, you could insert a dense layer (without bias) after the one-hot input layer, which will behave as an embedding layer (think about what calculation a dense layer actually performs, and how it behaves if the input is a one-hot vector: is there any difference with an embedding layer?).\n",
        " * Try windowed sampling instead of progressive sampling (this is usually **easier** to implement, but leads to slower sampling times and has a more limited receptive field).\n",
        " * Try top-K sampling, nucleus sampling or beam search.\n",
        " * Try single-loss training, or try multi-loss training but weight the loss linearly across the entire sequence, i.e. attach little weight to the first tokens, and more weight to tokens later in the sequence.\n",
        " * Try temporal convolutions.\n",
        " * Try generating text on a word level instead of the character level. For this to work well you will need to do some preprocessing of the text first: tokenization, removal of punctuation, conversion of all characters to lowercase, etc. Then proceed by making an indexed vocabulary, but replace all the words that occur less than e.g. 10 times by an `<UNK>` token (for \"unknown\"). Also alter the architecture of the model by including an embedding layer after the input layer, which will generally lead to better results. Training time of such a model will be longer than a character-level model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDhh6lw0mkkl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}